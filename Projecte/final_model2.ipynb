{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc6850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cargar datos preprocesados\n",
    "with open('data/dataset_tonality_chords.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Preparar encoder para acordes\n",
    "all_chords = list(set(c for song in data['songs'] for c in song['chords']))\n",
    "chord_encoder = LabelEncoder()\n",
    "chord_encoder.fit(all_chords)\n",
    "\n",
    "# Preparar encoder para tonalidades\n",
    "tonalities = list(set(song['tonality'] for song in data['songs']))\n",
    "tonality_encoder = LabelEncoder()\n",
    "tonality_encoder.fit(tonalities)\n",
    "\n",
    "# Parámetros\n",
    "SEQUENCE_LENGTH = 8  # Longitud de secuencias de entrada\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class ChordGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, tonality_size, pad_token, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # Embeddings\n",
    "        self.tonality_embedding = nn.Embedding(tonality_size, embedding_dim)\n",
    "        self.chord_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Decodificador LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim * 2, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Token especial para inicio de secuencia\n",
    "        self.sos_token = vocab_size  # Añadiremos esto al vocabulario\n",
    "\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size + 2,\n",
    "            embedding_dim,\n",
    "            padding_idx=pad_token\n",
    "        )\n",
    "        \n",
    "    def forward(self, tonality, padded_sequences, lengths, max_length=16, temperature=1.0):\n",
    "        # Crear mask para ignorar padding\n",
    "        mask = (padded_sequences != self.pad_token)\n",
    "        \n",
    "        # Embeddings\n",
    "        seq_emb = self.embedding(padded_sequences)\n",
    "        tonality_emb = self.tonality_embedding(tonality).unsqueeze(1)\n",
    "        tonality_emb = tonality_emb.expand(-1, seq_emb.size(1), -1)\n",
    "        \n",
    "        combined = torch.cat([seq_emb, tonality_emb], dim=-1)\n",
    "        \n",
    "        # Empacar secuencias para mejor eficiencia\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            combined,\n",
    "            lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=True\n",
    "        )\n",
    "\n",
    "        batch_size = tonality.size(0)\n",
    "        \n",
    "        # Embedding de la tonalidad\n",
    "        tonality_emb = self.tonality_embedding(tonality)  # (batch, emb_dim)\n",
    "        \n",
    "        # Inicializar secuencia con SOS\n",
    "        input_chord = torch.full((batch_size, 1), self.sos_token, \n",
    "                               dtype=torch.long, device=tonality.device)\n",
    "        outputs = []\n",
    "        \n",
    "        # Estado inicial (usamos la tonalidad como contexto)\n",
    "        h = tonality_emb.unsqueeze(0).repeat(2, 1, 1)  # Para LSTM bidireccional\n",
    "        c = torch.zeros_like(h)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Embedding del acorde actual\n",
    "            chord_emb = self.chord_embedding(input_chord[:, -1:])  # (batch, 1, emb_dim)\n",
    "            \n",
    "            # Combinar con tonalidad\n",
    "            combined = torch.cat([\n",
    "                chord_emb, \n",
    "                tonality_emb.unsqueeze(1).expand(-1, chord_emb.size(1), -1)\n",
    "            ], dim=-1)\n",
    "            \n",
    "            # Paso por LSTM\n",
    "            lstm_out, (h, c) = self.lstm(combined, (h, c))\n",
    "            \n",
    "            # Predecir siguiente acorde\n",
    "            logits = self.fc(lstm_out.squeeze(1))\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            next_chord = torch.multinomial(probs, 1)\n",
    "            \n",
    "            outputs.append(next_chord)\n",
    "            input_chord = torch.cat([input_chord, next_chord], dim=1)\n",
    "        \n",
    "        return torch.cat(outputs, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d383a6f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 50\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtonality\u001b[39m\u001b[38;5;124m'\u001b[39m: tonality,\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadded_sequences\u001b[39m\u001b[38;5;124m'\u001b[39m: sequences_padded,\n\u001b[1;32m     46\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlengths\u001b[39m\u001b[38;5;124m'\u001b[39m: lengths\n\u001b[1;32m     47\u001b[0m         }\n\u001b[1;32m     49\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TonalityDataset(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msongs\u001b[39m\u001b[38;5;124m'\u001b[39m], chord_encoder, tonality_encoder)\n\u001b[0;32m---> 50\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(\n\u001b[1;32m     51\u001b[0m     dataset,\n\u001b[1;32m     52\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     53\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     54\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcollate_fn  \u001b[38;5;66;03m# ¡Usamos nuestra función personalizada!\u001b[39;00m\n\u001b[1;32m     55\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TonalityDataset(Dataset):\n",
    "    def __init__(self, songs, chord_encoder, tonality_encoder, max_length=16):\n",
    "        self.songs = songs\n",
    "        self.chord_encoder = chord_encoder\n",
    "        self.tonality_encoder = tonality_encoder\n",
    "        self.max_length = max_length\n",
    "        self.sos_token = len(chord_encoder.classes_)\n",
    "        self.pad_token = self.sos_token + 1  # Nuevo token para padding\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.songs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        song = self.songs[idx]\n",
    "        chords = song['chords'][:self.max_length-1]  # -1 para el SOS\n",
    "        \n",
    "        # Codificar y añadir SOS\n",
    "        encoded = [self.sos_token] + self.chord_encoder.transform(chords).tolist()\n",
    "        \n",
    "        return {\n",
    "            'tonality': torch.tensor(self.tonality_encoder.transform([song['tonality']])[0]),\n",
    "            'sequence': torch.tensor(encoded),\n",
    "            'length': len(encoded)  # Guardamos la longitud real\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # Ordenar por longitud (descendente) para packed sequences\n",
    "        batch.sort(key=lambda x: x['length'], reverse=True)\n",
    "        \n",
    "        tonality = torch.stack([x['tonality'] for x in batch])\n",
    "        sequences = [x['sequence'] for x in batch]\n",
    "        lengths = torch.tensor([x['length'] for x in batch])\n",
    "        \n",
    "        # Aplicar padding\n",
    "        sequences_padded = pad_sequence(\n",
    "            sequences,\n",
    "            batch_first=True,\n",
    "            padding_value=self.pad_token\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'tonality': tonality,\n",
    "            'padded_sequences': sequences_padded,\n",
    "            'lengths': lengths\n",
    "        }\n",
    "    \n",
    "dataset = TonalityDataset(data['songs'], chord_encoder, tonality_encoder)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=dataset.collate_fn  # ¡Usamos nuestra función personalizada!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "757fef65",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ChordGenerator.__init__() got an unexpected keyword argument 'pad_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m PAD_TOKEN \u001b[38;5;241m=\u001b[39m VOCAB_SIZE \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# El último token es para padding\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Modelo\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mChordGenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVOCAB_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtonality_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTONALITY_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPAD_TOKEN\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Entrenamiento\u001b[39;00m\n\u001b[1;32m     75\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_model(model, dataloader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: ChordGenerator.__init__() got an unexpected keyword argument 'pad_token'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, dataloader, epochs=50, lr=0.001):\n",
    "    # Configuración inicial\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=model.pad_token)  # Ignora los tokens de padding\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Historial de métricas\n",
    "    train_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Barra de progreso\n",
    "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # 1. Preparar datos\n",
    "            tonalities = batch['tonality'].to(device)\n",
    "            sequences = batch['padded_sequences'].to(device)\n",
    "            lengths = batch['lengths'].to(device)\n",
    "            \n",
    "            # 2. Separar inputs (todo excepto último acorde) y targets (todo excepto primer SOS)\n",
    "            inputs = sequences[:, :-1]\n",
    "            targets = sequences[:, 1:]\n",
    "            \n",
    "            # 3. Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                tonality=tonalities,\n",
    "                padded_sequences=inputs,\n",
    "                lengths=lengths-1  # -1 porque quitamos un elemento\n",
    "            )\n",
    "            \n",
    "            # 4. Calcular pérdida (aplanamos las secuencias)\n",
    "            loss = criterion(\n",
    "                outputs.view(-1, outputs.size(-1)),  # (batch*seq_len, vocab_size)\n",
    "                targets.reshape(-1)                  # (batch*seq_len)\n",
    "            )\n",
    "            \n",
    "            # 5. Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 6. Actualizar métricas\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # 7. Guardar métricas de la época\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        train_loss.append(avg_loss)\n",
    "        print(f'Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "# Hiperparámetros\n",
    "VOCAB_SIZE = len(chord_encoder.classes_) + 2  # +2 para SOS y PAD\n",
    "TONALITY_SIZE = len(tonality_encoder.classes_)\n",
    "PAD_TOKEN = VOCAB_SIZE - 1  # El último token es para padding\n",
    "\n",
    "# Modelo\n",
    "model = ChordGenerator(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    tonality_size=TONALITY_SIZE,\n",
    "    pad_token=PAD_TOKEN\n",
    ")\n",
    "\n",
    "# Entrenamiento\n",
    "train_loss = train_model(model, dataloader, epochs=50)\n",
    "# Guardar el modelo\n",
    "torch.save(model.state_dict(), 'chord_generator.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
