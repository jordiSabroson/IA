{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Learn the Basics](intro.html) \\|\\|\n",
    "[Quickstart](quickstart_tutorial.html) \\|\\|\n",
    "[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n",
    "DataLoaders](data_tutorial.html) \\|\\|\n",
    "[Transforms](transforms_tutorial.html) \\|\\| **Build Model** \\|\\|\n",
    "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
    "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
    "Model](saveloadrun_tutorial.html)\n",
    "\n",
    "Build the Neural Network\n",
    "========================\n",
    "\n",
    "Neural networks comprise of layers/modules that perform operations on\n",
    "data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace\n",
    "provides all the building blocks you need to build your own neural\n",
    "network. Every module in PyTorch subclasses the\n",
    "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "A neural network is a module itself that consists of other modules\n",
    "(layers). This nested structure allows for building and managing complex\n",
    "architectures easily.\n",
    "\n",
    "In the following sections, we\\'ll build a neural network to classify\n",
    "images in the FashionMNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Device for Training\n",
    "=======================\n",
    "\n",
    "We want to be able to train our model on a hardware accelerator like the\n",
    "GPU or MPS, if available. Let\\'s check to see if\n",
    "[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) or\n",
    "[torch.backends.mps](https://pytorch.org/docs/stable/notes/mps.html) are\n",
    "available, otherwise we use the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Class\n",
    "================\n",
    "\n",
    "We define our neural network by subclassing `nn.Module`, and initialize\n",
    "the neural network layers in `__init__`. Every `nn.Module` subclass\n",
    "implements the operations on input data in the `forward` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of `NeuralNetwork`, and move it to the `device`,\n",
    "and print its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This executes the model\\'s\n",
    "`forward`, along with some [background\n",
    "operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n",
    "Do not call `model.forward()` directly!\n",
    "\n",
    "Calling the model on the input returns a 2-dimensional tensor with dim=0\n",
    "corresponding to each output of 10 raw predicted values for each class,\n",
    "and dim=1 corresponding to the individual values of each output. We get\n",
    "the prediction probabilities by passing it through an instance of the\n",
    "`nn.Softmax` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3.5568e-01, 2.7180e-01, 5.6373e-01, 2.4500e-01, 9.1931e-01,\n",
      "          5.6184e-01, 8.1010e-01, 9.3863e-01, 6.2271e-02, 6.7647e-01,\n",
      "          1.9217e-01, 9.3962e-01, 9.0039e-01, 2.2040e-01, 3.3428e-01,\n",
      "          5.0134e-01, 3.9216e-01, 3.0749e-02, 1.1921e-01, 7.2443e-01,\n",
      "          8.2987e-01, 6.2187e-01, 5.6035e-01, 9.8203e-01, 3.1890e-01,\n",
      "          3.1358e-03, 9.5026e-01, 8.1826e-03],\n",
      "         [5.6772e-03, 4.0883e-01, 6.4094e-01, 7.4965e-01, 3.9148e-01,\n",
      "          4.9656e-01, 2.1115e-01, 3.7513e-01, 8.5457e-01, 7.5076e-01,\n",
      "          9.6209e-01, 4.1814e-01, 5.0328e-01, 2.9652e-01, 9.7557e-01,\n",
      "          2.7621e-01, 2.2855e-01, 8.1416e-01, 6.3689e-01, 2.1736e-01,\n",
      "          1.7673e-01, 4.9862e-04, 6.4536e-01, 2.5573e-01, 4.9815e-01,\n",
      "          5.8634e-01, 9.2611e-01, 3.2190e-01],\n",
      "         [9.8041e-01, 3.8452e-01, 7.7122e-01, 2.9518e-01, 1.2140e-01,\n",
      "          2.0616e-01, 5.8197e-01, 2.1342e-02, 5.1038e-01, 9.8341e-01,\n",
      "          3.5087e-01, 8.3930e-03, 5.8868e-01, 4.9975e-01, 1.8977e-01,\n",
      "          6.3955e-01, 8.2316e-01, 7.2223e-01, 9.6661e-01, 3.2502e-01,\n",
      "          2.1698e-01, 2.1526e-01, 2.7434e-01, 8.0147e-01, 6.2472e-01,\n",
      "          7.4561e-01, 8.5052e-01, 3.2850e-01],\n",
      "         [3.2850e-01, 2.5943e-01, 8.2257e-01, 4.6583e-02, 7.3124e-01,\n",
      "          2.2112e-01, 6.0884e-01, 4.2334e-01, 7.8395e-01, 9.8570e-01,\n",
      "          1.5656e-01, 3.2571e-02, 8.4122e-01, 7.3299e-01, 9.3681e-01,\n",
      "          6.1647e-01, 4.4134e-01, 6.8831e-01, 2.5541e-01, 7.4104e-01,\n",
      "          6.8056e-01, 9.6223e-01, 4.0444e-01, 4.3977e-01, 4.1560e-02,\n",
      "          9.7053e-01, 2.6492e-01, 4.5243e-01],\n",
      "         [2.9448e-02, 8.9851e-01, 2.3949e-01, 5.0042e-01, 6.2224e-01,\n",
      "          4.9638e-01, 6.7805e-01, 1.8722e-01, 7.7927e-01, 1.6446e-02,\n",
      "          9.3374e-01, 9.0144e-01, 7.0417e-01, 2.6559e-01, 4.9401e-01,\n",
      "          4.7144e-01, 3.1093e-01, 9.1016e-01, 6.6441e-01, 7.1679e-01,\n",
      "          5.8097e-01, 8.9884e-01, 7.0648e-01, 3.2722e-01, 9.0937e-01,\n",
      "          1.8545e-02, 5.3458e-01, 7.1882e-01],\n",
      "         [6.9796e-01, 1.2912e-01, 8.8091e-02, 4.4657e-01, 5.3918e-01,\n",
      "          7.6605e-01, 5.9697e-01, 1.7365e-01, 9.3784e-01, 9.8051e-01,\n",
      "          9.6682e-01, 5.1821e-01, 6.0850e-01, 4.5505e-01, 4.5120e-01,\n",
      "          1.6655e-01, 2.8993e-01, 2.6097e-01, 5.3219e-01, 3.0225e-01,\n",
      "          6.3476e-01, 1.5484e-01, 7.0948e-01, 1.0384e-01, 5.2124e-01,\n",
      "          7.3712e-01, 5.3862e-01, 6.1124e-02],\n",
      "         [2.3878e-01, 7.1669e-01, 1.8375e-01, 9.0155e-01, 7.2702e-01,\n",
      "          5.0831e-01, 4.5989e-01, 4.2889e-01, 8.4623e-02, 6.2878e-01,\n",
      "          4.7586e-01, 5.8139e-01, 8.5921e-01, 6.4490e-01, 1.7708e-01,\n",
      "          7.4652e-01, 4.2920e-01, 8.8364e-01, 6.2322e-01, 6.7353e-01,\n",
      "          2.9160e-01, 1.1317e-01, 6.9427e-01, 7.5821e-02, 5.9889e-01,\n",
      "          6.6257e-01, 5.1541e-01, 8.8844e-01],\n",
      "         [2.0292e-01, 3.7719e-01, 9.9794e-01, 6.1239e-01, 6.9492e-01,\n",
      "          1.1591e-01, 7.6383e-01, 7.8173e-01, 1.9198e-01, 3.2179e-01,\n",
      "          2.9570e-01, 6.5858e-02, 3.2383e-01, 3.1244e-01, 8.0553e-01,\n",
      "          1.3146e-01, 6.5067e-01, 9.1905e-01, 7.1818e-01, 8.0373e-01,\n",
      "          2.0789e-01, 9.1902e-02, 1.9649e-01, 4.3266e-01, 1.0105e-01,\n",
      "          7.3859e-01, 5.1587e-03, 8.5826e-01],\n",
      "         [6.7348e-01, 6.1199e-01, 9.9867e-01, 5.6174e-01, 2.8892e-01,\n",
      "          8.1616e-01, 7.6428e-01, 5.7379e-01, 7.8770e-01, 9.5741e-01,\n",
      "          8.8271e-04, 5.7753e-01, 7.1145e-01, 2.4322e-01, 8.6087e-01,\n",
      "          5.6423e-01, 7.8073e-01, 2.9949e-01, 8.3071e-01, 1.8848e-01,\n",
      "          7.5702e-02, 5.2958e-01, 7.7381e-01, 7.2598e-01, 1.6014e-01,\n",
      "          1.5060e-01, 6.8074e-01, 5.8292e-01],\n",
      "         [4.2701e-01, 6.3291e-01, 6.0266e-01, 5.1096e-01, 2.7643e-01,\n",
      "          9.0458e-01, 5.8467e-01, 8.6621e-01, 6.6191e-01, 1.6357e-01,\n",
      "          3.5749e-01, 8.0270e-01, 5.0090e-01, 7.1372e-01, 6.2494e-01,\n",
      "          5.9396e-01, 5.8432e-02, 8.9409e-01, 5.0554e-01, 2.1750e-01,\n",
      "          2.4214e-01, 9.0395e-01, 7.5464e-01, 9.2906e-01, 3.6084e-01,\n",
      "          4.0331e-01, 3.4681e-01, 4.4976e-01],\n",
      "         [5.8714e-01, 1.3287e-01, 4.6870e-01, 4.3737e-02, 1.6440e-01,\n",
      "          1.2590e-01, 6.0710e-01, 2.7636e-01, 9.7826e-01, 9.2936e-01,\n",
      "          1.5604e-01, 8.5166e-01, 3.8664e-01, 9.6103e-01, 9.5418e-01,\n",
      "          2.3437e-01, 8.1060e-01, 1.1189e-02, 6.3127e-01, 4.5252e-01,\n",
      "          4.9647e-01, 1.0644e-01, 6.2825e-01, 4.2005e-01, 5.3654e-01,\n",
      "          9.7662e-01, 4.9333e-01, 5.4542e-01],\n",
      "         [1.6490e-01, 5.7786e-01, 1.6551e-01, 1.8469e-01, 5.6176e-01,\n",
      "          7.6968e-02, 2.1512e-01, 9.9715e-01, 7.0205e-01, 9.4773e-01,\n",
      "          5.9651e-01, 2.7844e-01, 2.9458e-01, 4.4518e-01, 1.1439e-02,\n",
      "          3.7362e-01, 4.3546e-01, 1.9847e-01, 1.3765e-01, 7.4901e-01,\n",
      "          2.4122e-01, 5.8882e-01, 1.2123e-01, 9.4145e-01, 7.4566e-01,\n",
      "          3.8758e-01, 4.1245e-01, 2.1146e-01],\n",
      "         [8.4687e-02, 1.2174e-01, 3.0812e-01, 9.0745e-01, 8.5115e-01,\n",
      "          3.4793e-01, 2.0407e-01, 2.5869e-01, 1.7063e-01, 2.4624e-01,\n",
      "          7.8434e-01, 9.8584e-01, 8.9346e-01, 8.2579e-01, 5.9729e-01,\n",
      "          6.7911e-01, 4.7486e-01, 9.7534e-01, 9.8768e-01, 5.8323e-01,\n",
      "          4.3500e-01, 4.4989e-01, 5.3480e-01, 5.4532e-01, 9.1399e-01,\n",
      "          2.9178e-01, 7.4773e-01, 2.0868e-01],\n",
      "         [7.1339e-01, 3.1904e-01, 8.6118e-01, 3.4308e-01, 8.4957e-01,\n",
      "          6.4489e-01, 7.7960e-01, 1.6574e-01, 9.8072e-01, 5.5390e-01,\n",
      "          5.5233e-01, 7.1691e-01, 3.0705e-01, 6.9775e-01, 5.0211e-01,\n",
      "          4.0424e-01, 5.7501e-01, 8.8020e-02, 5.8481e-02, 8.0348e-02,\n",
      "          4.6856e-01, 1.2732e-01, 8.5474e-01, 8.2307e-01, 3.0375e-02,\n",
      "          6.4868e-01, 9.2946e-01, 3.0393e-01],\n",
      "         [9.2089e-01, 3.8385e-01, 8.2726e-01, 4.8836e-01, 6.4010e-01,\n",
      "          9.4783e-01, 6.5125e-01, 9.4915e-01, 7.0240e-01, 7.0633e-01,\n",
      "          8.7553e-01, 6.3246e-01, 6.7805e-01, 4.5338e-01, 9.3802e-01,\n",
      "          1.3186e-01, 7.2667e-01, 6.1739e-01, 8.9116e-01, 7.4019e-01,\n",
      "          7.6350e-01, 3.4215e-01, 2.9259e-01, 6.9874e-01, 2.0989e-01,\n",
      "          4.7024e-01, 4.5908e-01, 6.7130e-01],\n",
      "         [9.1572e-02, 7.4300e-01, 6.3878e-02, 7.8968e-01, 9.0604e-01,\n",
      "          9.8958e-01, 1.2399e-01, 3.1090e-01, 3.2882e-01, 1.2265e-01,\n",
      "          3.2869e-01, 2.6432e-01, 8.4928e-01, 4.6681e-01, 9.6241e-01,\n",
      "          8.1002e-01, 4.2439e-02, 1.7489e-01, 8.7815e-01, 6.5960e-01,\n",
      "          2.5394e-01, 7.5660e-01, 1.4492e-01, 3.2740e-02, 4.7594e-01,\n",
      "          2.1665e-01, 7.9497e-01, 1.5525e-02],\n",
      "         [4.5683e-01, 8.7044e-01, 7.2678e-01, 6.6435e-01, 2.4686e-01,\n",
      "          1.2109e-01, 3.4656e-01, 5.8992e-01, 9.9734e-01, 5.6973e-01,\n",
      "          6.0943e-01, 8.2627e-01, 8.5732e-01, 1.2189e-01, 5.9799e-02,\n",
      "          8.0054e-01, 8.5667e-02, 7.1292e-01, 3.2523e-01, 6.2951e-01,\n",
      "          5.1671e-01, 8.3529e-01, 7.0137e-01, 5.4630e-02, 7.0022e-01,\n",
      "          1.7198e-01, 1.2599e-01, 9.0465e-01],\n",
      "         [4.0697e-02, 7.2098e-01, 7.5123e-01, 9.7673e-01, 2.1913e-01,\n",
      "          3.1733e-01, 7.3664e-01, 2.4377e-01, 1.9763e-01, 1.8295e-02,\n",
      "          2.3815e-02, 7.2005e-01, 6.7316e-01, 5.5022e-01, 5.1986e-01,\n",
      "          4.5541e-01, 5.3182e-02, 5.0107e-01, 7.5746e-01, 8.0436e-01,\n",
      "          1.7725e-01, 5.1184e-01, 6.0437e-01, 1.3921e-01, 2.4327e-01,\n",
      "          5.3042e-01, 6.7793e-01, 1.9736e-01],\n",
      "         [1.9835e-01, 8.1004e-01, 9.6332e-01, 8.9297e-01, 3.1584e-01,\n",
      "          5.1375e-01, 6.4877e-02, 1.9551e-01, 8.9302e-01, 1.2424e-01,\n",
      "          8.4604e-01, 5.8576e-01, 4.5527e-01, 4.9331e-01, 2.1473e-01,\n",
      "          9.4907e-01, 1.2159e-01, 8.4505e-02, 8.8053e-01, 6.4174e-01,\n",
      "          6.0294e-01, 9.6006e-01, 9.6700e-01, 9.0274e-01, 8.9468e-01,\n",
      "          2.6170e-01, 7.8610e-01, 1.1071e-01],\n",
      "         [8.2255e-01, 7.2428e-01, 4.7554e-02, 7.0428e-01, 1.4093e-01,\n",
      "          6.7414e-01, 9.3260e-01, 7.3494e-01, 2.9206e-01, 4.1793e-01,\n",
      "          5.3438e-01, 4.4437e-01, 1.0491e-01, 8.6582e-01, 1.1194e-01,\n",
      "          8.2822e-01, 2.0075e-01, 4.9623e-02, 2.5666e-01, 6.3319e-01,\n",
      "          3.6663e-01, 2.0660e-01, 8.3612e-01, 5.7602e-01, 8.4257e-01,\n",
      "          3.6546e-01, 7.9439e-01, 1.3734e-01],\n",
      "         [1.1926e-01, 1.4598e-01, 5.7461e-01, 3.9469e-01, 7.5122e-01,\n",
      "          2.4475e-01, 3.9415e-02, 4.2485e-01, 8.0513e-01, 5.6595e-01,\n",
      "          9.2179e-01, 5.1983e-01, 1.2946e-01, 9.0556e-01, 5.6148e-01,\n",
      "          6.0774e-01, 3.5940e-01, 2.7337e-01, 7.7160e-01, 6.9149e-01,\n",
      "          2.4756e-01, 4.1300e-01, 8.6266e-01, 1.4775e-01, 7.2662e-01,\n",
      "          2.5846e-01, 1.0681e-01, 4.1680e-02],\n",
      "         [7.2033e-01, 2.7715e-01, 1.0296e-01, 3.2602e-01, 2.3898e-01,\n",
      "          4.1335e-01, 7.3410e-02, 4.5826e-01, 4.1701e-01, 5.6068e-01,\n",
      "          7.7389e-01, 2.3947e-02, 3.8515e-01, 4.5954e-02, 9.6623e-01,\n",
      "          3.1299e-01, 1.8186e-01, 2.8028e-01, 8.4777e-01, 7.1648e-03,\n",
      "          6.7977e-01, 8.3005e-01, 2.3638e-01, 5.0232e-01, 8.8654e-01,\n",
      "          4.4977e-01, 7.7539e-01, 4.8049e-01],\n",
      "         [7.2458e-01, 1.4598e-01, 6.4008e-01, 2.7937e-01, 1.4211e-01,\n",
      "          1.3345e-01, 1.4921e-02, 4.7280e-01, 6.8390e-01, 1.5406e-01,\n",
      "          1.6471e-01, 4.4657e-01, 2.5739e-01, 2.9275e-01, 9.8337e-01,\n",
      "          5.8769e-01, 3.9483e-01, 3.4887e-01, 4.2636e-02, 7.0113e-01,\n",
      "          3.4275e-02, 9.0250e-01, 1.7596e-02, 7.0202e-01, 2.9121e-01,\n",
      "          2.5028e-01, 5.5522e-01, 5.4113e-01],\n",
      "         [2.7138e-01, 6.9598e-02, 3.5139e-01, 5.4366e-01, 4.8644e-01,\n",
      "          7.8600e-01, 6.6643e-01, 7.0755e-01, 6.9945e-01, 5.5888e-01,\n",
      "          3.2364e-01, 1.8529e-01, 2.7422e-01, 6.5726e-01, 2.7432e-01,\n",
      "          5.3235e-01, 5.7971e-01, 6.3251e-01, 4.8671e-01, 4.7188e-02,\n",
      "          4.0288e-01, 8.6399e-01, 7.2148e-01, 4.3834e-01, 6.0734e-01,\n",
      "          2.3798e-01, 8.6295e-01, 6.8420e-01],\n",
      "         [5.8804e-01, 8.0994e-01, 8.4415e-01, 3.9966e-01, 1.2152e-01,\n",
      "          3.0983e-01, 1.0325e-01, 6.8161e-01, 9.0036e-01, 3.9800e-01,\n",
      "          9.0347e-01, 8.1115e-01, 2.8844e-01, 7.2066e-01, 2.3978e-02,\n",
      "          2.5422e-01, 6.1732e-01, 4.4740e-01, 7.4545e-01, 4.1635e-01,\n",
      "          7.0680e-01, 4.9368e-01, 5.3229e-01, 4.5258e-01, 5.4590e-01,\n",
      "          9.0632e-01, 4.4389e-01, 5.6102e-02],\n",
      "         [5.3068e-03, 7.6633e-01, 2.3454e-01, 4.2006e-01, 5.2375e-01,\n",
      "          4.6462e-04, 6.1697e-01, 7.4271e-01, 6.4826e-01, 1.1341e-01,\n",
      "          7.9506e-01, 6.8546e-02, 4.8912e-01, 3.4586e-01, 6.9952e-01,\n",
      "          8.1165e-01, 6.3121e-01, 8.4958e-01, 1.0707e-01, 9.3910e-01,\n",
      "          1.7700e-01, 2.5381e-03, 2.6718e-01, 6.4953e-02, 5.6742e-02,\n",
      "          9.7064e-01, 4.1127e-01, 2.0044e-01],\n",
      "         [6.8359e-01, 9.8909e-01, 2.7702e-01, 8.8082e-01, 3.3091e-01,\n",
      "          5.0813e-01, 4.6366e-01, 3.6229e-01, 6.1198e-01, 2.6612e-01,\n",
      "          4.6813e-01, 3.1659e-01, 5.6959e-01, 1.9031e-01, 3.8391e-01,\n",
      "          7.8221e-01, 3.6873e-01, 2.2689e-01, 7.8588e-01, 3.2593e-01,\n",
      "          5.4416e-01, 3.7274e-02, 2.1666e-01, 5.3790e-01, 7.0330e-01,\n",
      "          9.2742e-01, 6.2417e-01, 8.3906e-01],\n",
      "         [4.8446e-01, 6.2177e-01, 2.9945e-01, 3.0943e-01, 5.1971e-01,\n",
      "          6.8292e-01, 2.2214e-01, 9.1179e-02, 9.9938e-01, 2.6505e-02,\n",
      "          9.6367e-01, 3.3581e-01, 1.6069e-01, 3.3679e-01, 1.0198e-01,\n",
      "          9.4144e-01, 8.8129e-01, 7.9208e-01, 6.7046e-01, 1.9668e-01,\n",
      "          8.6410e-01, 7.5108e-01, 8.5094e-01, 7.3808e-01, 9.0613e-01,\n",
      "          9.2229e-01, 8.4877e-01, 7.7062e-01]]], device='cuda:0')\n",
      "Predicted class: tensor([6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "print(X)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Layers\n",
    "============\n",
    "\n",
    "Let\\'s break down the layers in the FashionMNIST model. To illustrate\n",
    "it, we will take a sample minibatch of 3 images of size 28x28 and see\n",
    "what happens to it as we pass it through the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Flatten\n",
    "==========\n",
    "\n",
    "We initialize the\n",
    "[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n",
    "layer to convert each 2D 28x28 image into a contiguous array of 784\n",
    "pixel values ( the minibatch dimension (at dim=0) is maintained).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear\n",
    "=========\n",
    "\n",
    "The [linear\n",
    "layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "is a module that applies a linear transformation on the input using its\n",
    "stored weights and biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.ReLU\n",
    "=======\n",
    "\n",
    "Non-linear activations are what create the complex mappings between the\n",
    "model\\'s inputs and outputs. They are applied after linear\n",
    "transformations to introduce *nonlinearity*, helping neural networks\n",
    "learn a wide variety of phenomena.\n",
    "\n",
    "In this model, we use\n",
    "[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "between our linear layers, but there\\'s other activations to introduce\n",
    "non-linearity in your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.4565, -0.1312,  0.3884, -0.2676,  0.2631,  0.1213,  0.5622, -0.3696,\n",
      "         -0.2753, -0.0062, -0.2187, -0.0516,  0.0022,  0.1075,  0.3042, -0.2665,\n",
      "         -0.2184, -0.1354, -0.0346,  0.9301],\n",
      "        [-0.1222, -0.0902,  0.0412, -0.4622,  0.1229, -0.2050,  0.7770, -0.2813,\n",
      "         -0.1728,  0.4880,  0.0491, -0.3908,  0.1625, -0.2729,  0.2139, -0.0023,\n",
      "          0.0456,  0.2518,  0.1366,  0.3197],\n",
      "        [-0.1233, -0.4204,  0.6116,  0.2102,  0.1018,  0.0362,  0.4355, -0.0485,\n",
      "         -0.6380, -0.0269, -0.0313, -0.2464, -0.1325, -0.0880,  0.2883, -0.2398,\n",
      "          0.0397,  0.1373, -0.0653,  0.6727]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.3884, 0.0000, 0.2631, 0.1213, 0.5622, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0022, 0.1075, 0.3042, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.9301],\n",
      "        [0.0000, 0.0000, 0.0412, 0.0000, 0.1229, 0.0000, 0.7770, 0.0000, 0.0000,\n",
      "         0.4880, 0.0491, 0.0000, 0.1625, 0.0000, 0.2139, 0.0000, 0.0456, 0.2518,\n",
      "         0.1366, 0.3197],\n",
      "        [0.0000, 0.0000, 0.6116, 0.2102, 0.1018, 0.0362, 0.4355, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2883, 0.0000, 0.0397, 0.1373,\n",
      "         0.0000, 0.6727]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential\n",
    "=============\n",
    "\n",
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "is an ordered container of modules. The data is passed through all the\n",
    "modules in the same order as defined. You can use sequential containers\n",
    "to put together a quick network like `seq_modules`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Softmax\n",
    "==========\n",
    "\n",
    "The last linear layer of the neural network returns [logits]{.title-ref}\n",
    "- raw values in \\[-infty, infty\\] - which are passed to the\n",
    "[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)\n",
    "module. The logits are scaled to values \\[0, 1\\] representing the\n",
    "model\\'s predicted probabilities for each class. `dim` parameter\n",
    "indicates the dimension along which the values must sum to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters\n",
    "================\n",
    "\n",
    "Many layers inside a neural network are *parameterized*, i.e. have\n",
    "associated weights and biases that are optimized during training.\n",
    "Subclassing `nn.Module` automatically tracks all fields defined inside\n",
    "your model object, and makes all parameters accessible using your\n",
    "model\\'s `parameters()` or `named_parameters()` methods.\n",
    "\n",
    "In this example, we iterate over each parameter, and print its size and\n",
    "a preview of its values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0275,  0.0120, -0.0097,  ..., -0.0185, -0.0072,  0.0146],\n",
      "        [ 0.0010, -0.0347,  0.0064,  ...,  0.0318,  0.0217, -0.0027]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0102,  0.0098], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0014, -0.0153,  0.0316,  ..., -0.0332,  0.0415, -0.0244],\n",
      "        [-0.0400, -0.0265,  0.0255,  ...,  0.0240,  0.0110, -0.0284]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0119, -0.0175], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0420, -0.0392,  0.0127,  ..., -0.0094,  0.0070, -0.0377],\n",
      "        [ 0.0360, -0.0176,  0.0205,  ...,  0.0171, -0.0192, -0.0210]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0120,  0.0239], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading\n",
    "===============\n",
    "\n",
    "-   [torch.nn API](https://pytorch.org/docs/stable/nn.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
